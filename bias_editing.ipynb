{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8812879",
   "metadata": {},
   "source": [
    "# Activation steering by editing the bias in gpt2-xl\n",
    "\n",
    "This notebook shows how instead of adding a steering vector to the internal activations we can add it to the model bias of the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d78bb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from baukit import Trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "556075bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "522544c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2-xl\").to(device).eval()\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2-xl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4681e32",
   "metadata": {},
   "source": [
    "Instead of adding the steering vector to the activation vector we can add the product of next-layer-weights and steering vector to the next-layer-bias.\n",
    "Note, we can only do this if our layer has the structure $\\phi_{l+1} = \\sigma(W_l \\phi_l + b_l)$.\n",
    "\n",
    "This is generally not the case for the residual stream. However we do find this structure in the attention and MLP layers. \n",
    "\n",
    "Lets say we want to edit the bias in the attention module in layer 5. This is equivalent to adding a steering vector to the output of the previous module, here the layernorm.\n",
    "As a first step we thus have to get the layernorm output of layer 5. Here we use baukit to get it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f44b76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "act_love.shape: torch.Size([1, 1, 1600])\n",
      "act_hate.shape: torch.Size([1, 2, 1600])\n"
     ]
    }
   ],
   "source": [
    "# define layer to do the activation steering on\n",
    "layer_id = 5\n",
    "module = model.transformer.h[layer_id].ln_1\n",
    "\n",
    "# get internal activations\n",
    "inputs = tokenizer(\"Love\", return_tensors=\"pt\").to(device)\n",
    "with Trace(module) as cache:\n",
    "    _ = model(**inputs)\n",
    "    act_love = cache.output\n",
    "\n",
    "inputs = tokenizer(\"Hate\", return_tensors=\"pt\").to(device)\n",
    "with Trace(module) as cache:\n",
    "    _ = model(**inputs)\n",
    "    act_hate = cache.output\n",
    "\n",
    "print(f\"act_love.shape: {act_love.shape}\")\n",
    "print(f\"act_hate.shape: {act_hate.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b32f66b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steering_vec.shape:  torch.Size([1, 1, 1600])\n",
      "length steering_vec: 18.80\n"
     ]
    }
   ],
   "source": [
    "# define the steering vector\n",
    "steering_vec = act_love[:,-1:,:]-act_hate[:,-1:,:]\n",
    "print(f\"steering_vec.shape:  {steering_vec.shape}\")\n",
    "print(f\"length steering_vec: {steering_vec.norm():.2f}\")\n",
    "\n",
    "# reset the steering vector length to 1\n",
    "steering_vec /= steering_vec.norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7c13db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "org_bias.shape: torch.Size([4800])\n"
     ]
    }
   ],
   "source": [
    "# lets save the original bias value\n",
    "org_bias = model.transformer.h[layer_id].attn.c_attn.bias\n",
    "print(f\"org_bias.shape: {org_bias.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f462c1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions for resetting and setting the bias\n",
    "def reset_bias_attn(model, layer_id, org_bias):\n",
    "\tmodel.transformer.h[layer_id].attn.c_attn.bias = org_bias\n",
    "\t\n",
    "def change_bias_attn(model, layer_id, steering_vec):\n",
    "\ttilde_b = model.transformer.h[layer_id].attn.c_attn(steering_vec.squeeze())\n",
    "\tmodel.transformer.h[layer_id].attn.c_attn.bias = torch.nn.parameter.Parameter(tilde_b)\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1c4f7440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think dogs are  a great example of how we can use technology\n",
      "--------------------\n",
      "I think dogs are icky, but I don't think they're \n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"I think dogs are \"\n",
    "\n",
    "# generate text while steering in positive direction\n",
    "coeff = 10\n",
    "change_bias_attn(model, layer_id, coeff*steering_vec)\n",
    "inputs = tokenizer(test_sentence, return_tensors=\"pt\").to(device)\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=10, pad_token_id=tokenizer.eos_token_id, do_sample=False)\n",
    "generated_text = tokenizer.batch_decode(generated_ids)\n",
    "reset_bias_attn(model, layer_id, org_bias)\n",
    "print(generated_text[0])\n",
    "print(\"-\"*20)\n",
    "\n",
    "# generate text while steering in negative direction\n",
    "coeff = -10\n",
    "change_bias_attn(model, layer_id, coeff*steering_vec)\n",
    "inputs = tokenizer(test_sentence, return_tensors=\"pt\").to(device)\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=10, pad_token_id=tokenizer.eos_token_id, do_sample=False)\n",
    "generated_text = tokenizer.batch_decode(generated_ids)\n",
    "reset_bias_attn(model, layer_id, org_bias)\n",
    "print(generated_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "df14c38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think dogs are  a great way to get your dog to learn\n"
     ]
    }
   ],
   "source": [
    "# generate text without steering\n",
    "inputs = tokenizer(test_sentence, return_tensors=\"pt\").to(device)\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=10, pad_token_id=tokenizer.eos_token_id, do_sample=False)\n",
    "generated_text = tokenizer.batch_decode(generated_ids)\n",
    "print(generated_text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94590f8a",
   "metadata": {},
   "source": [
    "Editing the bias in this way in the attention module in layer 5 is equivalent to adding the steering vector to the output of the layernorm module in layer 5 as demonstrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fed515c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think dogs are  a great example of how we can use technology\n",
      "--------------------\n",
      "I think dogs are icky, but I don't think they're \n"
     ]
    }
   ],
   "source": [
    "# define the activation steering funtion\n",
    "def act_add(steering_vec):\n",
    "    def hook(output):\n",
    "        # the output of the layernorm module is not a tuple\n",
    "        return output + steering_vec\n",
    "    return hook\n",
    "\n",
    "# generate text while steering in positive direction\n",
    "coeff = 10\n",
    "inputs = tokenizer(test_sentence, return_tensors=\"pt\").to(device)\n",
    "with Trace(module, edit_output=act_add(coeff*steering_vec)) as _:\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=10, pad_token_id=tokenizer.eos_token_id, do_sample=False)\n",
    "    generated_text = tokenizer.batch_decode(generated_ids)\n",
    "print(generated_text[0])\n",
    "print(\"-\"*20)\n",
    "\n",
    "# generate text while steering in negative direction\n",
    "coeff = -10\n",
    "inputs = tokenizer(test_sentence, return_tensors=\"pt\").to(device)\n",
    "with Trace(module, edit_output=act_add(coeff*steering_vec)) as _:\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=10, pad_token_id=tokenizer.eos_token_id, do_sample=False)\n",
    "    generated_text = tokenizer.batch_decode(generated_ids)\n",
    "    \n",
    "print(generated_text[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
