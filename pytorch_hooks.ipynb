{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to how to do activation addition with pytorch hooks\n",
    "\n",
    "This notebook shows how to access and modify internal model activations using pytorch hooks. Let's start with loading model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "dtype = torch.bfloat16\n",
    "# we want the padding side to be left as we want an easy way to access the last token\n",
    "padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.49s/it]\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model_name = \"llama-7b\"\n",
    "model_path = f\"huggyllama/{model_name}\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path).to(device, dtype=dtype)\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.padding_side = padding_side\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access hidden activations\n",
    "We can access hidden activations of the residual streem by passing the `output_hidden_states` keyword. But if we want other hidden states we need some way to hook into the other transformer layers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['logits', 'past_key_values', 'hidden_states'])\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    inputs = tokenizer(test_sentence, return_tensors=\"pt\").to(device)\n",
    "    output = model(**inputs, output_hidden_states=True)\n",
    "    print(output.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len hidden states: 33\n",
      "shape of hidden states: torch.Size([1, 12, 4096])\n"
     ]
    }
   ],
   "source": [
    "print(f\"len hidden states: {len(output.hidden_states)}\")\n",
    "print(f\"shape of hidden states: {output.hidden_states[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the residual layer output of layer layer_id is\n",
    "# output.hidden_states[layer_id + 1] as the first hidden state is the input embedding\n",
    "layer_id = 5\n",
    "hidden_states = output.hidden_states[layer_id + 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing hidden states with pytorch hooks\n",
    "Instead of passing the `output_hidden_states=True` keyword to the forward pass which outputs ALL hidden states, we can hook into a specific module and save these specific hidden states.\n",
    "\n",
    "In order to do that we need to define a pytorch hook. As we are interested in getting/changing information in the forward pass we will be using [forward hooks](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_forward_hook)\n",
    "\n",
    "A hook has the following signature:\n",
    "```\n",
    "hook(module, input, output) -> None or modified output\n",
    "```\n",
    "We can then attach the hook by calling `register_forward_hook` on a `torch.nn.Module`, like for example on the 5th decoder layer of a Llama model. The `register_forward_hook` returns a handle that can be used to remove the added hook by calling `handle.remove()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a hook to save internal model activations\n",
    "def cache_activations(cache):\n",
    "    def hook(module, input, output):\n",
    "        if isinstance(output, tuple):\n",
    "            cache[:] = output[0]\n",
    "        else:\n",
    "            cache[:] = output\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE between cached_activations and hidden_stated: 0.0\n"
     ]
    }
   ],
   "source": [
    "cached_activations = torch.zeros_like(hidden_states)\n",
    "hook_handle = model.model.layers[layer_id].register_forward_hook(cache_activations(cached_activations))\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = tokenizer(test_sentence, return_tensors=\"pt\").to(device)\n",
    "    output = model(**inputs)\n",
    "\n",
    "hook_handle.remove()\n",
    "\n",
    "print(f\"MSE between cached_activations and hidden_stated: {(hidden_states-cached_activations).pow(2).mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With this we can also access other hidden representations, like the attention layer for example\n",
    "If I do not know the size of the object I want to cache, I can simply append it to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of cached activations: torch.Size([1, 12, 4096])\n"
     ]
    }
   ],
   "source": [
    "# define our activation cache hook\n",
    "def cache_activations_list(cache):\n",
    "    def hook(module, input, output):\n",
    "        # for some layer (for example decoder layer in Llama) the output contains additional info besides activations\n",
    "        if isinstance(output, tuple):\n",
    "            cache.append(output[0])\n",
    "        else:\n",
    "            cache.append(output)\n",
    "    return hook\n",
    "\n",
    "cached_activations = []\n",
    "hook_handle = model.model.layers[layer_id].self_attn.q_proj.register_forward_hook(cache_activations_list(cached_activations))\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = tokenizer(test_sentence, return_tensors=\"pt\").to(device)\n",
    "    output = model(**inputs)\n",
    "\n",
    "hook_handle.remove()\n",
    "print(f\"shape of cached activations: {cached_activations[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation addition with Pytorch hooks\n",
    "\n",
    "Define a direction vector and add it to the internal model activations while generating new model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our addition hook\n",
    "def add_to_activations(toadd):\n",
    "    def hook(module, input, output):\n",
    "        # for some layer (for example decoder layer in Llama) the output contains additional info besides activations\n",
    "        if isinstance(output, tuple):\n",
    "            # the output cannot be modifies in place, we actually have to return the modified output\n",
    "            return (output[0] + toadd,) + output[1:]\n",
    "        else:\n",
    "            return output + toadd\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_with_cache(model, tokenizer, sentences, device=\"cuda\", hidden_size=model.config.hidden_size, \n",
    "                    module_to_hook=model.model.layers[layer_id], hook_fun=cache_activations):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True).to(device)\n",
    "        # define a tensor with the size of our cached activations\n",
    "        cached_activations = torch.zeros(inputs[\"input_ids\"].shape + (hidden_size,), device=device)\n",
    "        hook_handle = module_to_hook.register_forward_hook(hook_fun(cached_activations))\n",
    "        output = model(**inputs)\n",
    "\n",
    "    hook_handle.remove()\n",
    "\n",
    "    return cached_activations\n",
    "\n",
    "def generate_with_aa(model, tokenizer, sentences, direction, max_new_tokens=20, device=\"cuda\", random_seed=0,\n",
    "                    hidden_size=model.config.hidden_size, module_to_hook=model.model.layers[layer_id], hook_fun=add_to_activations):\n",
    "\n",
    "    hook_handle = module_to_hook.register_forward_hook(hook_fun(direction))\n",
    "    torch.random.manual_seed(random_seed)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True).to(device)\n",
    "        generate_ids = model.generate(**inputs, max_new_tokens=max_new_tokens, use_cache=True)\n",
    "        generated_text = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "\n",
    "    hook_handle.remove()\n",
    "    return generated_text\n",
    "\n",
    "def generate(model, tokenizer, sentences, direction, max_new_tokens=20, device=\"cuda\", random_seed=0):\n",
    "    torch.random.manual_seed(random_seed)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True).to(device)\n",
    "        generate_ids = model.generate(**inputs, max_new_tokens=max_new_tokens, use_cache=True)\n",
    "        generated_text = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape cached_activations: torch.Size([2, 3, 4096])\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"Love\", \"Hate\"]\n",
    "\n",
    "cached_activations = run_with_cache(model, tokenizer, sentences, device, \n",
    "                hidden_size=model.config.hidden_size, \n",
    "                module_to_hook=model.model.layers[layer_id], \n",
    "                hook_fun=cache_activations)\n",
    "\n",
    "print(f\"shape cached_activations: {cached_activations.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of direction: torch.Size([1, 1, 4096])\n",
      "norm of direction:  16.25\n"
     ]
    }
   ],
   "source": [
    "token_pos = -1\n",
    "# the direction should have the same number of dimensions as the activations (they usually have shape [batch_size, num_tokens, hidden_dim])\n",
    "# the easiest is just to define a direction with shape [1, 1, hidden_dim]\n",
    "# this can then be added to all tokens for the complete batch\n",
    "direction = cached_activations[0:1, token_pos:, :] - cached_activations[1:, token_pos:, :]\n",
    "# make sure the direction vector is on the same device and has same precision as the model\n",
    "direction = direction.to(device=device, dtype=dtype)\n",
    "print(f\"shape of direction: {direction.shape}\")\n",
    "print(f\"norm of direction:  {direction.norm(dim=-1).item():.4g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate with positive direction:\n",
      "\n",
      "I think dogs are the best thing in the world.\n",
      "I love my dogs, but I love my dogs more.\n",
      "---\n",
      "I think cats are the best.\n",
      "I love my cats. I love my cats.\n",
      "I love my\n",
      "---\n",
      "Today I feel like I’m in a bit of a rut with my hair. I’ve been using\n",
      "---\n",
      "\n",
      "Generate with negative direction:\n",
      "\n",
      "I think dogs are the worst.\n",
      "I'm not a dog person, but I've never liked them.\n",
      "---\n",
      "I think cats are the most evil creatures on the planet. I hate them. I hate them. I hate them\n",
      "---\n",
      "Today I feel like I've been doing this for a while. I've been trying to get back into\n",
      "---\n",
      "\n",
      "Generate without vector addition:\n",
      "\n",
      "I think dogs are the best. I have a dog named Lucky. He is a 10 year old ch\n",
      "---\n",
      "I think cats are the most beautiful animals in the world. I love them so much. I have a cat named T\n",
      "---\n",
      "Today I feel like I’m in a bit of a rut. I’m not sure what to write\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"I think dogs are\", \"I think cats are\", \"Today I feel\"]\n",
    "random_seed = 0\n",
    "\n",
    "generated_love = generate_with_aa(model, tokenizer, sentences, direction, max_new_tokens=20,\n",
    "                                    device=\"cuda\", random_seed=random_seed, hidden_size=model.config.hidden_size, \n",
    "                                    module_to_hook=model.model.layers[layer_id], hook_fun=add_to_activations)\n",
    "\n",
    "print(\"Generate with positive direction:\\n\")\n",
    "for sentence in generated_love:\n",
    "    print(sentence)\n",
    "    print(\"---\")\n",
    "\n",
    "\n",
    "generated_hate = generate_with_aa(model, tokenizer, sentences, -direction, max_new_tokens=20,\n",
    "                                    device=\"cuda\", random_seed=random_seed, hidden_size=model.config.hidden_size, \n",
    "                                    module_to_hook=model.model.layers[layer_id], hook_fun=add_to_activations)\n",
    "\n",
    "print(\"\\nGenerate with negative direction:\\n\")\n",
    "for sentence in generated_hate:\n",
    "    print(sentence)\n",
    "    print(\"---\")\n",
    "\n",
    "\n",
    "generated_neutral = generate(model, tokenizer, sentences, -direction, max_new_tokens=20,\n",
    "                                    device=\"cuda\", random_seed=random_seed)\n",
    "\n",
    "print(\"\\nGenerate without vector addition:\\n\")\n",
    "for sentence in generated_neutral:\n",
    "    print(sentence)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "* you need to keep track of the hook handles. It can be a bit annoying (especially when developing) if you lose one of the handles and then you have to reload the model -> some hook handle tracker could help out (one could implement a class or a decorator to do this)\n",
    "* make sure that tensors are on the same device and use the same precision\n",
    "* there are unfortunately many differences in how models are implemented, so you might have to adapt parameter names like `model.config.hidden_size` or module names like `model.model.layers[layer_id]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
