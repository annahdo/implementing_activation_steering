{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation steering with TransformerLens and gpt2-xl\n",
    "\n",
    "This notebook shows how to access and modify internal model activations using the transformer lens library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformer_lens import HookedTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-xl into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "# load transformer lens model\n",
    "model = HookedTransformer.from_pretrained_no_processing(\"gpt2-xl\", default_prepend_bos=False).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "act_love.shape: torch.Size([1, 1, 1600])\n",
      "act_hate.shape: torch.Size([1, 2, 1600])\n"
     ]
    }
   ],
   "source": [
    "# define what layer/module you want information from and get the internal activations\n",
    "layer_id = 5\n",
    "cache_name = f\"blocks.{layer_id}.hook_resid_post\" # we do activation steering on the activation (the output) of the residual layer\n",
    "\n",
    "_, cache = model.run_with_cache(\"Love\")\n",
    "act_love = cache[cache_name]\n",
    "_, cache = model.run_with_cache(\"Hate\")\n",
    "act_hate = cache[cache_name]\n",
    "\n",
    "print(f\"act_love.shape: {act_love.shape}\")\n",
    "print(f\"act_hate.shape: {act_hate.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see by looking at the shape of the activation tensors, the input \"sentences\" are tokenized into different numbers of tokens. To make this into a vector we only take the numerical values of the last token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steering_vec.shape:  torch.Size([1, 1, 1600])\n",
      "length steering_vec: 100.23\n"
     ]
    }
   ],
   "source": [
    "# define the steering vector\n",
    "steering_vec = act_love[:,-1:,:]-act_hate[:,-1:,:]\n",
    "print(f\"steering_vec.shape:  {steering_vec.shape}\")\n",
    "print(f\"length steering_vec: {steering_vec.norm():.2f}\")\n",
    "\n",
    "# reset the steering vector length to 1\n",
    "steering_vec /= steering_vec.norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the activation steering funtion\n",
    "def act_add(steering_vec):\n",
    "    def hook(activation, hook):\n",
    "        return activation + steering_vec\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We previously used the function `run_with_cache` to get the internal activations. This function adds PyTorch hooks before running the model and removes them afterwards.\n",
    "There is also the function `run_with_hooks` for which you can set your own hook functions. However I did not find a function `generate_with_hooks`.\n",
    "\n",
    "If we want to generate new text, the model needs to repeatedly perform a forward pass and we want our activation addition to happen in each forward pass. We consequently need to set a hook that does the activation addition. After we generated our text it is important to remove the hook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:03<00:00,  3.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think dogs are  a great way to get to know someone.\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:03<00:00,  3.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think dogs are icky, but I don't think they're \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"I think dogs are \"\n",
    "\n",
    "# generate text while steering in positive direction\n",
    "coeff = 10\n",
    "model.add_hook(name=cache_name, hook=act_add(coeff*steering_vec))\n",
    "print(model.generate(test_sentence, max_new_tokens=10, do_sample=False))\n",
    "model.reset_hooks()\n",
    "print(\"-\"*20)\n",
    "\n",
    "# generate text while steering in negative direction\n",
    "coeff = -10\n",
    "test_sentence = \"I think dogs are \"\n",
    "model.add_hook(name=cache_name, hook=act_add(coeff*steering_vec))\n",
    "print(model.generate(test_sentence, max_new_tokens=10, do_sample=False))\n",
    "model.reset_hooks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  3.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think dogs are  a great way to get your dog to learn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# generate text without steering\n",
    "print(model.generate(test_sentence, max_new_tokens=10, do_sample=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the HookedTransformer model is different to the output of the basemodel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from baukit import Trace\n",
    "\n",
    "# load model\n",
    "org_model = AutoModelForCausalLM.from_pretrained(\"gpt2-xl\").to(device).eval()\n",
    "# load tokenizer\n",
    "org_tokenizer = AutoTokenizer.from_pretrained(\"gpt2-xl\", add_bos_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think dogs are  a great way to get your dog to learn\n"
     ]
    }
   ],
   "source": [
    "# generate text without steering\n",
    "inputs = org_tokenizer(test_sentence, return_tensors=\"pt\").to(device)\n",
    "generated_ids = org_model.generate(**inputs, max_new_tokens=10, pad_token_id=org_tokenizer.eos_token_id, do_sample=False)\n",
    "generated_text = org_tokenizer.batch_decode(generated_ids)\n",
    "print(generated_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare internal model activations between the HookedTransformer model and the original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse org_activation, baukit_activation: 0.0\n"
     ]
    }
   ],
   "source": [
    "# define layer to do the activation steering on\n",
    "layer_id = 5\n",
    "module = org_model.transformer.h[layer_id]\n",
    "test_sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# get internal activations\n",
    "inputs = org_tokenizer(test_sentence, return_tensors=\"pt\").to(device)\n",
    "with Trace(module) as cache:\n",
    "    _ = org_model(**inputs)\n",
    "    baukit_activation = cache.output[0]\n",
    "\n",
    "\n",
    "org_activation = org_model(**inputs, output_hidden_states=True)[\"hidden_states\"][layer_id+1] # hidden_states[0] are the input embeddings\n",
    "\n",
    "print(f\"mse org_activation, baukit_activation: {(baukit_activation-org_activation).pow(2).mean()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, cache = model.run_with_cache(test_sentence)\n",
    "cache_name = f\"blocks.{layer_id}.hook_resid_post\"\n",
    "_, cache = model.run_with_cache(test_sentence)\n",
    "tl_activation = cache[cache_name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse org_activation, tl_activation: 2.3551237457626606e-13\n"
     ]
    }
   ],
   "source": [
    "print(f\"mse org_activation, tl_activation: {(baukit_activation-tl_activation).pow(2).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HookedTransformer(\n",
       "  (embed): Embed()\n",
       "  (hook_embed): HookPoint()\n",
       "  (pos_embed): PosEmbed()\n",
       "  (hook_pos_embed): HookPoint()\n",
       "  (blocks): ModuleList(\n",
       "    (0-47): 48 x TransformerBlock(\n",
       "      (ln1): LayerNorm(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (ln2): LayerNorm(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (attn): Attention(\n",
       "        (hook_k): HookPoint()\n",
       "        (hook_q): HookPoint()\n",
       "        (hook_v): HookPoint()\n",
       "        (hook_z): HookPoint()\n",
       "        (hook_attn_scores): HookPoint()\n",
       "        (hook_pattern): HookPoint()\n",
       "        (hook_result): HookPoint()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (hook_pre): HookPoint()\n",
       "        (hook_post): HookPoint()\n",
       "      )\n",
       "      (hook_attn_in): HookPoint()\n",
       "      (hook_q_input): HookPoint()\n",
       "      (hook_k_input): HookPoint()\n",
       "      (hook_v_input): HookPoint()\n",
       "      (hook_mlp_in): HookPoint()\n",
       "      (hook_attn_out): HookPoint()\n",
       "      (hook_mlp_out): HookPoint()\n",
       "      (hook_resid_pre): HookPoint()\n",
       "      (hook_resid_mid): HookPoint()\n",
       "      (hook_resid_post): HookPoint()\n",
       "    )\n",
       "  )\n",
       "  (ln_final): LayerNorm(\n",
       "    (hook_scale): HookPoint()\n",
       "    (hook_normalized): HookPoint()\n",
       "  )\n",
       "  (unembed): Unembed()\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
